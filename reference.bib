@article{fuzzingstateofart,
abstract = {As one of the most popular software testing techniques, fuzzing can find a variety of weaknesses in a program, such as software bugs and vulnerabilities, by generating numerous test inputs. Due to its effectiveness, fuzzing is regarded as a valuable bug hunting method. In this paper, we present an overview of fuzzing that concentrates on its general process, as well as classifications, followed by detailed discussion of the key obstacles and some state-of-the-art technologies which aim to overcome or mitigate these obstacles. We further investigate and classify several widely used fuzzing tools. Our primary goal is to equip the stakeholder with a better understanding of fuzzing and the potential solutions for improving fuzzing methods in the spectrum of software testing and security. To inspire future research, we also predict some future directions with regard to fuzzing.},
author = {Liang, Hongliang and Pei, Xiaoxiao and Jia, Xiaodong and Shen, Wuwei and Zhang, Jian},
doi = {10.1109/TR.2018.2834476},
issn = {0018-9529},
journal = {IEEE Transactions on Reliability},
keywords = {Fuzzing,reliability,security,software testing,survey},
month = {sep},
number = {3},
pages = {1199--1218},
title = {{Fuzzing: State of the Art}},
volume = {67},
year = {2018}
}
@inproceedings{VUzzer,
address = {Reston, VA},
annote = {控制流和数据流指导fuzz},
author = {Rawat, Sanjay and Jain, Vivek and Kumar, Ashish and Cojocar, Lucian and Giuffrida, Cristiano and Bos, Herbert},
booktitle = {Proceedings of the 2017 Network and Distributed System Security Symposium},
doi = {10.14722/ndss.2017.23404},
isbn = {1-891562-46-0},
publisher = {Internet Society},
title = {{VUzzer: Application-aware Evolutionary Fuzzing}},
year = {2017}
}
@inproceedings{imageNet,
abstract = {It is common practice to decay the learning rate. Here we show one can usually obtain the same learning curve on both training and test sets by instead increasing the batch size during training. This procedure is successful for stochastic gradient descent (SGD), SGD with momentum, Nesterov momentum, and Adam. It reaches equivalent test accuracies after the same number of training epochs, but with fewer parameter updates, leading to greater parallelism and shorter training times. We can further reduce the number of parameter updates by increasing the learning rate {\$}\backslashepsilon{\$} and scaling the batch size {\$}B \backslashpropto \backslashepsilon{\$}. Finally, one can increase the momentum coefficient {\$}m{\$} and scale {\$}B \backslashpropto 1/(1-m){\$}, although this tends to slightly reduce the test accuracy. Crucially, our techniques allow us to repurpose existing training schedules for large batch training with no hyper-parameter tuning. We train ResNet-50 on ImageNet to {\$}76.1\backslash{\%}{\$} validation accuracy in under 30 minutes.},
address = {New York, New York, USA},
annote = {批处理训练},
author = {You, Yang and Zhang, Zhao and Hsieh, Cho-Jui and Demmel, James and Keutzer, Kurt},
booktitle = {Proceedings of the 47th International Conference on Parallel Processing},
doi = {10.1145/3225058.3225069},
isbn = {9781450365109},
month = {nov},
pages = {1--10},
publisher = {ACM Press},
title = {{ImageNet Training in Minutes}},
year = {2018}
}
@inproceedings{taintStyle,
author = {Yamaguchi, Fabian and Maier, Alwin and Gascon, Hugo and Rieck, Konrad},
booktitle = {Proceedings of the 2015 IEEE Symposium on Security and Privacy},
doi = {10.1109/SP.2015.54},
isbn = {978-1-4673-6949-7},
month = {may},
pages = {797--812},
publisher = {IEEE},
title = {{Automatic Inference of Search Patterns for Taint-Style Vulnerabilities}},
year = {2015}
}
@article{4+1view,
author = {Kruchten, P.B.},
doi = {10.1109/52.469759},
issn = {07407459},
journal = {IEEE Software},
number = {6},
pages = {42--50},
publisher = {IEEE},
title = {{The 4+1 View Model of architecture}},
volume = {12},
year = {1995}
}
@inproceedings{leopard,
abstract = {Identifying potentially vulnerable locations in a code base is critical as a pre-step for effective vulnerability assessment; i.e., it can greatly help security experts put their time and effort to where it is needed most. Metric-based and pattern-based methods have been presented for identifying vulnerable code. The former relies on machine learning and cannot work well due to the severe imbalance between non-vulnerable and vulnerable code or lack of features to characterize vulnerabilities. The latter needs the prior knowledge of known vulnerabilities and can only identify similar but not new types of vulnerabilities. In this paper, we propose and implement a generic, lightweight and extensible framework, LEOPARD, to identify potentially vulnerable functions through program metrics. LEOPARD requires no prior knowledge about known vulnerabilities. It has two steps by combining two sets of systematically derived metrics. First, it uses complexity metrics to group the functions in a target application into a set of bins. Then, it uses vulnerability metrics to rank the functions in each bin and identifies the top ones as potentially vulnerable. Our experimental results on 11 real-world projects have demonstrated that, LEOPARD can cover 74.0{\%} of vulnerable functions by identifying 20{\%} of functions as vulnerable and outperform machine learning-based and static analysis-based techniques. We further propose three applications of LEOPARD for manual code review and fuzzing, through which we discovered 22 new bugs in real applications like PHP, radare2 and FFmpeg, and eight of them are new vulnerabilities.},
archivePrefix = {arXiv},
arxivId = {1901.11479},
author = {Du, Xiaoning and Chen, Bihuan and Li, Yuekang and Guo, Jianmin and Zhou, Yaqin and Liu, Yang and Jiang, Yu},
booktitle = {Proceedings of the 2019 IEEE/ACM 41st International Conference on Software Engineering},
doi = {10.1109/ICSE.2019.00024},
eprint = {1901.11479},
isbn = {978-1-7281-0869-8},
month = {may},
pages = {60--71},
publisher = {IEEE},
title = {{LEOPARD: Identifying Vulnerable Code for Vulnerability Assessment Through Program Metrics}},
year = {2019}
}
@inproceedings{atp:escjava,
abstract = {Software development and maintenance are costly endeavors. The cost can be reduced if more software defects are detected earlier in the development cycle. This paper introduces the Extended Static Checker for Java (ESC/Java), an experimental compile-time program checker that finds common programming errors. The checker is powered by verification-condition generation and automatic theoremproving techniques. It provides programmers with a simple annotation language with which programmer design decisions can be expressed formally. ESC/Java examines the annotated software and warns of inconsistencies between the design decisions recorded in the annotations and the actual code, and also warns of potential runtime errors in the code. This paper gives an overview of the checker architecture and annotation language and describes our experience applying the checker to tens of thousands of lines of Java programs. Copyright 2002 ACM.},
address = {New York, New York, USA},
author = {Flanagan, Cormac and Leino, K. Rustan M. and Lillibridge, Mark and Nelson, Greg and Saxe, James B. and Stata, Raymie},
booktitle = {Proceedings of the ACM SIGPLAN 2002 Conference on Programming language design and implementation},
doi = {10.1145/512529.512558},
isbn = {1581134630},
issn = {03621340},
keywords = {Checking,Compile-time,Program},
month = {jul},
number = {4S},
pages = {234},
publisher = {ACM Press},
title = {{Extended static checking for Java}},
volume = {48},
year = {2002}
}
@inproceedings{sym:dart,
abstract = {We present a new tool, named DART, for automatically testing software that combines three main techniques: (1) automated extraction of the interface of a program with its external environment using static source-code parsing; (2) automatic generation of a test driver for this interface that performs random testing to simulate the most general environment the program can operate in; and (3) dynamic analysis of how the program behaves under random testing and automatic generation of new test inputs to direct systematically the execution along alternative program paths. Together, these three techniques constitute Directed Automated Random Testing, or DART for short. The main strength of DART is thus that testing can be performed completely automatically on any program that compiles - there is no need to write any test driver or harness code. During testing, DART detects standard errors such as program crashes, assertion violations, and non-termination. Preliminary experiments to unit test several examples of C programs are very encouraging. Copyright 2005 ACM.},
address = {New York, New York, USA},
author = {Godefroid, Patrice and Klarlund, Nils and Sen, Koushik},
booktitle = {Proceedings of the 2005 ACM SIGPLAN conference on Programming language design and implementation},
doi = {10.1145/1065010.1065036},
isbn = {1595930566},
issn = {03621340},
keywords = {Automated Test Generation,Interfaces,Program Verification,Random Testing,Software Testing},
number = {6},
pages = {213},
publisher = {ACM Press},
title = {{DART: directed automated random testing}},
volume = {40},
year = {2005}
}
@inproceedings{vuldeepecker,
abstract = {The automatic detection of software vulnerabilities is an important research problem. However, existing solutions to this problem rely on human experts to define features and often miss many vulnerabilities (i.e., incurring high false negative rate). In this paper, we initiate the study of using deep learning-based vulnerability detection to relieve human experts from the tedious and subjective task of manually defining features. Since deep learning is motivated to deal with problems that are very different from the problem of vulnerability detection, we need some guiding principles for applying deep learning to vulnerability detection. In particular, we need to find representations of software programs that are suitable for deep learning. For this purpose, we propose using code gadgets to represent programs and then transform them into vectors, where a code gadget is a number of (not necessarily consecutive) lines of code that are semantically related to each other. This leads to the design and implementation of a deep learning-based vulnerability detection system, called Vulnerability Deep Pecker (VulDeePecker). In order to evaluate VulDeePecker, we present the first vulnerability dataset for deep learning approaches. Experimental results show that VulDeePecker can achieve much fewer false negatives (with reasonable false positives) than other approaches. We further apply VulDeePecker to 3 software products (namely Xen, Seamonkey, and Libav) and detect 4 vulnerabilities, which are not reported in the National Vulnerability Database but were "silently" patched by the vendors when releasing later versions of these products; in contrast, these vulnerabilities are almost entirely missed by the other vulnerability detection systems we experimented with.},
address = {Reston, VA},
archivePrefix = {arXiv},
arxivId = {1801.01681},
author = {Li, Zhen and Zou, Deqing and Xu, Shouhuai and Ou, Xinyu and Jin, Hai and Wang, Sujuan and Deng, Zhijun and Zhong, Yuyi},
booktitle = {Proceedings of the 2018 Network and Distributed System Security Symposium},
doi = {10.14722/ndss.2018.23158},
eprint = {1801.01681},
isbn = {1-891562-49-5},
publisher = {Internet Society},
title = {{VulDeePecker: A Deep Learning-Based System for Vulnerability Detection}},
year = {2018}
}
@inproceedings{atp:infer,
abstract = {The accurate and efficient treatment of mutable data structures is one of the outstanding problem areas in automatic program verification and analysis. Shape analysis is a form of program analysis that attempts to infer descriptions of the data structures in a program, and to prove that these structures are not misused or corrupted. It is one of the more challenging and expensive forms of program analysis, due to the complexity of aliasing and the need to look arbitrarily deeply into the program heap. This article describes a method of boosting shape analyses by defining a compositional method, where each procedure is analyzed independently of its callers. The analysis algorithm uses a restricted fragment of separation logic, and assigns a collection of Hoare triples to each procedure; the triples provide an over-approximation of data structure usage. Our method brings the usual benefits of compositionality- increased potential to scale, ability to deal with incomplete programs, graceful way to deal with imprecision-to shape analysis, for the first time. The analysis rests on a generalized form of abduction (inference of explanatory hypotheses), which we call bi-abduction. Bi-abduction displays abduction as a kind of inverse to the frame problem: it jointly infers anti-frames (missing portions of state) and frames (portions of state not touched by an operation), and is the basis of a new analysis algorithm. We have implemented our analysis and we report case studies on smaller programs to evaluate the quality of discovered specifications, and larger code bases (e.g., sendmail, an imap server, a Linux distribution) to illustrate the level of automation and scalability that we obtain from our compositional method. {\textcopyright} 2011.},
address = {New York, New York, USA},
author = {Calcagno, Cristiano and Distefano, Dino and O'Hearn, Peter and Yang, Hongseok},
booktitle = {Proceedings of the 36th annual ACM SIGPLAN-SIGACT symposium on Principles of programming languages},
doi = {10.1145/1480881.1480917},
isbn = {9781605583792},
issn = {00045411},
keywords = {Abstract interpretation,Compositionality,Program proving,Separation logic,Static analysis},
month = {dec},
number = {6},
pages = {289},
publisher = {ACM Press},
title = {{Compositional shape analysis by means of bi-abduction}},
volume = {58},
year = {2008}
}
@inproceedings{ayukselCaseStudy,
author = {Yuksel, Ulas and Sozer, Hasan},
booktitle = {Proceedings of the 2013 IEEE International Conference on Software Maintenance},
doi = {10.1109/ICSM.2013.89},
isbn = {978-0-7695-4981-1},
month = {sep},
pages = {532--535},
publisher = {IEEE},
title = {{Automated Classification of Static Code Analysis Alerts: A Case Study}},
year = {2013}
}
@article{taint:finding,
abstract = {Many techniques have been developed over the years to automatically find bugs in software. Often, these techniques rely on formal methods and sophisticated program analysis. While these techniques are valuable, they can be difficult to apply, and they aren't always effective in finding real bugs. Bug patterns are code idioms that are often errors. We have implemented automatic detectors for a variety of bug patterns found in Java programs. In this extended abstract1, we describe how we have used bug pattern detectors to find serious bugs in several widely used Java applications and libraries. We have found that the effort required to implement a bug pattern detector tends to be low, and that even extremely simple detectors find bugs in real applications. From our experience applying bug pattern detectors to real programs, we have drawn several interesting conclusions. First, we have found that even well tested code written by experts contains a surprising number of obvious bugs. Second, Java (and similar languages) have many language features and APIs which are prone to misuse. Finally, that simple automatic techniques can be effective at countering the impact of both ordinary mistakes and misunderstood language features.},
address = {New York, New York, USA},
author = {Hovemeyer, David and Pugh, William},
doi = {10.1145/1028664.1028717},
isbn = {1581138334},
journal = {Companion to the 19th annual ACM SIGPLAN conference on Object-oriented programming systems, languages, and applications},
keywords = {Bug checkers,Bug patterns,Bugs,Static analysis},
number = {12},
pages = {132},
publisher = {ACM Press},
title = {{Finding bugs is easy}},
volume = {39},
year = {2004}
}
@inproceedings{sym:klee,
abstract = {We present a new symbolic execution tool, KLEE, capable of automatically generating tests that achieve high coverage on a diverse set of complex and environmentally-intensive programs. We used KLEE to thoroughly check all 89 stand-alone programs in the GNU COREUTILS utility suite, which form the core user-level environment installed on millions of Unix systems, and arguably are the single most heavily tested set of open-source programs in existence. KLEE-generated tests achieve high line coverage - on average over 90{\%} per tool (median: over 94{\%}) - and significantly beat the coverage of the developers' own hand-written test suite. When we did the same for 75 equivalent tools in the BUSYBOX embedded system suite, results were even better, including 100{\%} coverage on 31 of them. We also used KLEE as a bug finding tool, applying it to 452 applications (over 430K total lines of code), where it found 56 serious bugs, including three in COREUTILS that had been missed for over 15 years. Finally, we used KLEE to crosscheck purportedly identical BUSYBOX and COREUTILS utilities, finding functional correctness errors and a myriad of inconsistencies.},
author = {Cadar, Cristian and Dunbar, Daniel and Engler, Dawson},
booktitle = {Proceedings of the 8th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2008},
doi = {10.5555/1855741.1855756},
isbn = {9781931971652},
pages = {209--224},
title = {{Klee: Unassisted and automatic generation of high-coverage tests for complex systems programs}},
volume = {8},
year = {2019}
}
@inproceedings{vuddy,
author = {Kim, Seulbae and Woo, Seunghoon and Lee, Heejo and Oh, Hakjoo},
booktitle = {Proceedings of the 2017 IEEE Symposium on Security and Privacy},
doi = {10.1109/SP.2017.62},
isbn = {978-1-5090-5533-3},
month = {may},
pages = {595--614},
publisher = {IEEE},
title = {{VUDDY: A Scalable Approach for Vulnerable Code Clone Discovery}},
year = {2017}
}
@inproceedings{lstm:translate,
author = {Su, Jinsong and Tan, Zhixing and Xiong, Deyi and Ji, Rongrong and Shi, Xiaodong and Liu, Yang},
booktitle = {Proceedings of the 31st AAAI Conference on Artificial Intelligence},
pages = {3302--3308},
publisher = {AAAI Press},
series = {AAAI'17},
title = {{Lattice-Based Recurrent Neural Network Encoders for Neural Machine Translation}},
year = {2017}
}
@inproceedings{simtree,
address = {New York, New York, USA},
author = {Pham, Nam H. and Nguyen, Tung Thanh and Nguyen, Hoan Anh and Nguyen, Tien N.},
booktitle = {Proceedings of the 2010 IEEE/ACM international conference on Automated software engineering},
doi = {10.1145/1858996.1859089},
isbn = {9781450301169},
pages = {447},
publisher = {ACM Press},
title = {{Detection of recurring software vulnerabilities}},
year = {2010}
}
@incollection{Shastry2016,
annote = {数据流分析应用},
author = {Shastry, Bhargava and Yamaguchi, Fabian and Rieck, Konrad and Seifert, Jean-Pierre},
booktitle = {Detection of Intrusions and Malware, and Vulnerability Assessment},
doi = {10.1007/978-3-319-40667-1_5},
pages = {78--97},
publisher = {Springer, Cham},
title = {{Towards Vulnerability Discovery Using Staged Program Analysis}},
year = {2016}
}
@inproceedings{zranking,
address = {San Diego, CA, USA},
author = {Kremenek, Ted and Engler, Dawson},
booktitle = {Proceedings of the 10th International Conference on Static Analysis},
doi = {10.5555/1760267.1760289},
pages = {295--315},
title = {{Z-Ranking: Using Statistical Analysis to Counter the Impact of Static Analysis Approximations}},
year = {2003}
}
@article{atp:saturn,
abstract = {This article presents Saturn, a general framework for building precise and scalable static error detection systems. Saturn exploits recent advances in Boolean satisfiability (SAT) solvers and is path sensitive, precise down to the bit level, and models pointers and heap data. Our approach is also highly scalable, which we achieve using two techniques. First, for each program function, several optimizations compress the size of the Boolean formulas that model the control flow and data flow and the heap locations accessed by a function. Second, summaries in the spirit of type signatures are computed for each function, allowing interprocedural analysis without a dramatic increase in the size of the Boolean constraints to be solved. We have experimentally validated our approach by conducting two case studies involving a Linux lock checker and a memory leak checker. Results from the experiments show that our system scales well, parallelizes well, and finds more errors with fewer false positives than previous static error detection systems. {\textcopyright} 2007 ACM.},
author = {Xie, Yichen and Aiken, Alex},
doi = {10.1145/1232420.1232423},
issn = {01640925},
journal = {ACM Transactions on Programming Languages and Systems},
keywords = {Boolean satisfiability,Error detection,Program analysis},
month = {may},
number = {3},
pages = {16--es},
publisher = {ACM New York, NY, USA},
title = {{Saturn: A scalable framework for error detection using Boolean satisfiability}},
volume = {29},
year = {2007}
}
@inproceedings{naturalSoftware,
author = {Hindle, Abram and Barr, Earl T. and Su, Zhendong and Gabel, Mark and Devanbu, Premkumar},
booktitle = {2012 34th International Conference on Software Engineering (ICSE)},
doi = {10.1109/ICSE.2012.6227135},
isbn = {978-1-4673-1066-6},
month = {jun},
pages = {837--847},
publisher = {IEEE},
title = {{On the naturalness of software}},
year = {2012}
}
@article{lstm:1997,
author = {Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
doi = {10.1162/neco.1997.9.8.1735},
issn = {0899-7667},
journal = {Neural Computation},
month = {nov},
number = {8},
pages = {1735--1780},
publisher = {MIT Press},
title = {{Long Short-Term Memory}},
volume = {9},
year = {1997}
}
@article{falseAlarm,
author = {Kim, Youil and Lee, Jooyong and Han, Hwansoo and Choe, Kwang-Moo},
doi = {10.1016/j.infsof.2009.10.004},
issn = {09505849},
journal = {Information and Software Technology},
month = {feb},
number = {2},
pages = {210--219},
title = {{Filtering false alarms of buffer overflow analysis using SMT solvers}},
volume = {52},
year = {2010}
}
@article{slices:horwitz1990,
abstract = {A slice of a program with respect to a program point p and variable x consists of all statements of the program that might affect the value of x at point p. This paper concerns the problem of interprocedural slicing — generating a slice of an entire program, where the slice crosses the boundaries of procedure calls. To solve this problem, we introduce a new kind of graph to represent programs, called a system dependence graph, which extends previous dependence representations to incorporate collections of procedures (with procedure calls) rather than just monolithic programs. Our main result is an algorithm for interprocedural slicing that uses the new representation. The chief difficulty in interprocedural slicing is correctly accounting for the calling context of a called procedure. To handle this problem, system dependence graphs include some data-dependence edges that represent transitive dependencies due to the effects of procedure calls, in addition to the conventional direct-dependence edges. These edges are constructed with the aid of an auxiliary structure that represents calling and parameter-linkage relationships. This structure takes the form of an attribute grammar. The step of computing the required transitive-dependence edges is reduced to the construction of the subordinate characteristic graphs for the grammar's nonterminals. {\textcopyright} 1988, ACM. All rights reserved.},
author = {Horwitz, Susan and Reps, Thomas and Binkley, David},
doi = {10.1145/77606.77608},
issn = {0164-0925},
journal = {ACM Transactions on Programming Languages and Systems (TOPLAS)},
month = {jan},
number = {1},
pages = {26--60},
publisher = {ACM New York, NY, USA},
title = {{Interprocedural slicing using dependence graphs}},
volume = {12},
year = {1990}
}
@article{Koc2019,
abstract = {Despite their ability to detect critical bugs in software, developers consider high false positive rates to be a key barrier to using static analysis tools in practice. To improve the usability of these tools, researchers have recently begun to apply machine learning techniques to classify and filter false positive analysis reports. Although initial results have been promising, the long-term potential and best practices for this line of research are unclear due to the lack of detailed, large-scale empirical evaluation. To partially address this knowledge gap, we present a comparative empirical study of four machine learning techniques, namely hand-engineered features, bag of words, recurrent neural networks, and graph neural networks, for classifying false positives, using multiple ground-truth program sets. We also introduce and evaluate new data preparation routines for recurrent neural networks and node representations for graph neural networks, and show that these routines can have a substantial positive impact on classification accuracy. Overall, our results suggest that recurrent neural networks (which learn over a program's source code) outperform the other subject techniques, although interesting tradeoffs are present among all techniques. Our observations provide insight into the future research needed to speed the adoption of machine learning approaches in practice.},
author = {Koc, Ugur and Wei, Shiyi and Foster, Jeffrey S. and Carpuat, Marine and Porter, Adam A.},
doi = {10.1109/ICST.2019.00036},
isbn = {978-1-7281-1736-2},
journal = {Proceedings of the 12th IEEE Conference on Software Testing, Validation and Verification},
keywords = {False positive classification,Machine learning,Static analysis},
month = {apr},
pages = {288--299},
publisher = {IEEE},
title = {{An Empirical Assessment of Machine Learning Approaches for Triaging Reports of a Java Static Analysis Tool}},
year = {2019}
}
@inproceedings{trainLonger,
address = {Red Hook, NY, USA},
annote = {批量训练},
author = {Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
isbn = {9781510860964},
pages = {1729--1739},
publisher = {Curran Associates Inc.},
series = {NIPS'17},
title = {{Train Longer, Generalize Better: Closing the Generalization Gap in Large Batch Training of Neural Networks}},
year = {2017}
}
@article{slices:weiser1979,
abstract = {Seminal work on program slicing, one of the first works on$\backslash$nautomatic program decomposition for PU and debugging.$\backslash$nDescribes experiments.},
author = {Weiser, Mark},
doi = {10.1109/TSE.1984.5010248},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
month = {jul},
number = {4},
pages = {352--357},
title = {{Program Slicing}},
volume = {SE-10},
year = {1984}
}
@inproceedings{pixy,
annote = {污点分析},
author = {Jovanovic, N. and Kruegel, C. and Kirda, E.},
booktitle = {Proceedings of the 2006 IEEE Symposium on Security and Privacy},
doi = {10.1109/SP.2006.29},
isbn = {0-7695-2574-1},
pages = {6 pp.--263},
publisher = {IEEE},
title = {{Pixy: a static analysis tool for detecting Web application vulnerabilities}},
year = {2006}
}
@inproceedings{lstm:repo,
author = {White, Martin and Vendome, Christopher and Linares-Vasquez, Mario and Poshyvanyk, Denys},
booktitle = {Proceedings of the 12th Working Conference on Mining Software Repositories},
doi = {10.1109/MSR.2015.38},
isbn = {978-0-7695-5594-2},
month = {may},
pages = {334--345},
publisher = {IEEE},
title = {{Toward Deep Learning Software Repositories}},
year = {2015}
}
@inproceedings{falsepositive,
author = {Johnson, Brittany and Song, Yoonki and Murphy-Hill, Emerson and Bowdidge, Robert},
booktitle = {Proceedings of the 35th International Conference on Software Engineering (ICSE)},
doi = {10.1109/ICSE.2013.6606613},
isbn = {978-1-4673-3076-3},
month = {may},
pages = {672--681},
publisher = {IEEE},
title = {{Why don't software developers use static analysis tools to find bugs?}},
year = {2013}
}
@article{taint:taj,
abstract = {Taint analysis, a form of information-flow analysis, establishes whether values from untrusted methods and parameters may flow into security-sensitive operations. Taint analysis can detect many common vulnerabilities in Web applications, and so has attracted much attention from both the research community and industry. However, most static taint-analysis tools do not address critical requirements for an industrial-strength tool. Specifically, an industrial-strength tool must scale to large industrial Web applications, model essential Web-application code artifacts, and generate consumable reports for a wide range of attack vectors. We have designed and implemented a static Taint Analysis for Java (TAJ) that meets the requirements of industry-level applications. TAJ can analyze applications of virtually any size, as it employs a set of techniques designed to produce useful answers given limited time and space. TAJ addresses a wide variety of attack vectors, with techniques to handle reflective calls, flow through containers, nested taint, and issues in generating useful reports. This paper provides a description of the algorithms comprising TAJ, evaluates TAJ against production-level benchmarks, and compares it with alternative solutions. Copyright {\textcopyright} 2009 ACM.},
address = {New York, New York, USA},
author = {Tripp, Omer and Pistoia, Marco and Fink, Stephen J. and Sridharan, Manu and Weisman, Omri},
doi = {10.1145/1542476.1542486},
isbn = {9781605583921},
journal = {Proceedings of the 2009 ACM SIGPLAN conference on Programming language design and implementation},
keywords = {Information flow,Integrity,Security,Static analysis,Taint analysis,Web applications},
number = {6},
pages = {87},
publisher = {ACM Press},
title = {{TAJ: effective taint analysis of web applications}},
volume = {44},
year = {2009}
}
@inproceedings{lstm:gradient,
address = {Red Hook, NY, USA},
author = {Hanin, Boris},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
doi = {10.5555/3326943.3326997},
pages = {580--589},
publisher = {Curran Associates Inc.},
series = {NIPS'18},
title = {{Which Neural Net Architectures Give Rise to Exploding and Vanishing Gradients?}},
year = {2018}
}
@inproceedings{li2016vulpecker,
abstract = {Software vulnerabilities are the fundamental cause of many attacks. Even with rapid vulnerability patching, the problem is more complicated than it looks. One reason is that instances of the same vulnerability may exist in multiple software copies that are difficult to track in real life (e.g., different versions of libraries and applications). This calls for tools that can automatically search for vulnerable software with respect to a given vulnerability. In this paper, we move a step forward in this direction by presenting Vulnerability Pecker (VulPecker), a system for automatically detecting whether a piece of software source code contains a given vulnerability or not. The key insight underlying VulPecker is to leverage (i) a set of features that we define to characterize patches, and (ii) code-similarity algorithms that have been proposed for various purposes, while noting that no single code-similarity algorithm is effective for all kinds of vulnerabilities. Experiments show that VulPecker detects 40 vulnerabilities that are not published in the National Vulnerability Database (NVD). Among these vulnerabilities, 18 are not known for their existence and have yet to be confirmed by vendors at the time of writing (these vulnerabilities are "anonymized" in the present paper for ethical reasons), and the other 22 vulnerabilities have been "silently" patched by the vendors in the later releases of the vulnerable products.},
author = {Li, Zhen and Zou, Deqing and Xu, Shouhuai and Jin, Hai and Qi, Hanchao and Hu, Jie},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/2991079.2991102},
isbn = {9781450347716},
keywords = {Code similarity,Vulnerability detection,Vulnerability signature},
pages = {201--213},
title = {{VulPecker: An automated vulnerability detection system based on code similarity analysis}},
volume = {5-9-Decemb},
year = {2016}
}
@inproceedings{Driller,
address = {Reston, VA},
annote = {符号执行制导fuzz},
author = {Stephens, Nick and Grosen, John and Salls, Christopher and Dutcher, Andrew and Wang, Ruoyu and Corbetta, Jacopo and Shoshitaishvili, Yan and Kruegel, Christopher and Vigna, Giovanni},
booktitle = {Proceedings of the 2016 Network and Distributed System Security Symposium},
doi = {10.14722/ndss.2016.23368},
isbn = {1-891562-41-X},
publisher = {Internet Society},
title = {{Driller: Augmenting Fuzzing Through Selective Symbolic Execution}},
year = {2016}
}
@inproceedings{lstm:recognize,
address = {USA},
author = {Shin, Eui Chul Richard and Song, Dawn and Moazzezi, Reza},
booktitle = {Proceedings of the 24th USENIX Conference on Security Symposium},
isbn = {9781931971232},
pages = {611--626},
publisher = {USENIX Association},
series = {SEC'15},
title = {{Recognizing Functions in Binaries with Neural Networks}},
year = {2015}
}
@article{slices:xu2005brief,
abstract = {Program slicing is a technique to extract program parts with re- spect to some special computation. Since Weiser first proposed the notion of slicing in 1979, hundreds of papers have been presented in this area. Tens of variants of slicing have been studied, as well as algorithms to compute them. Different notions of slicing have different properties and different applications. These notions vary from Weiser's syntax-preserving static slicing to amorphous slic- ing which is not syntax-preserving, and the algorithms can be based on dataflow equations, information-flow relations or depend- ence graphs. Slicing was first developed to facilitate debugging, but it is then found helpful in many aspects of the software development life cycle, including program debugging, software testing, software measurement, program comprehension, software maintenance, program parallelization and so on. Over the last two decades, several surveys on program slicing have been presented. However, most of them only reviewed parts of researches on program slicing or have now been out of date. Peo- ple who are interested in program slicing need more information about the up to date researches. Our survey fills this gap. In this paper, we briefly review most of existing slicing techniques in- cluding static slicing, dynamic slicing and the latest slicing tech- niques. We also discuss the contribution of each work and compare the major difference between them. Researches on slicing are classified by the research hot spots such that people can be kept informed of the overall program slicing researches.},
author = {Xu, Baowen and Qian, Ju and Zhang, Xiaofang and Wu, Zhongqiang and Chen, Lin},
doi = {10.1145/1050849.1050865},
issn = {01635948},
journal = {ACM SIGSOFT Software Engineering Notes},
number = {2},
pages = {1},
publisher = {ACM New York, NY, USA},
title = {{A brief survey of program slicing}},
volume = {30},
year = {2005}
}
@inproceedings{CodeAlchemist,
abstract = {JavaScript engines are an attractive target for attackers due to their popularity and flexibility in building exploits. Current state-of-the-art fuzzers for finding JavaScript engine vulnerabilities focus mainly on generating syntactically correct test cases based on either a predefined context-free grammar or a trained probabilistic language model. Unfortunately , syntactically correct JavaScript sentences are often semantically invalid at runtime. Furthermore, statically analyzing the semantics of JavaScript code is challenging due to its dynamic nature: JavaScript code is generated at runtime, and JavaScript expressions are dynamically-typed. To address this challenge, we propose a novel test case generation algorithm that we call semantics-aware assembly, and implement it in a fuzz testing tool termed CodeAlchemist. Our tool can generate arbitrary JavaScript code snippets that are both semantically and syntactically correct, and it effectively yields test cases that can crash JavaScript engines. We found numerous vulnerabilities of the latest JavaScript engines with CodeAlchemist and reported them to the vendors.},
address = {Reston, VA},
author = {Han, HyungSeok and Oh, DongHyeon and Cha, Sang Kil},
booktitle = {Proceedings of the 2019 Network and Distributed System Security Symposium},
doi = {10.14722/ndss.2019.23263},
isbn = {1-891562-55-X},
number = {February},
publisher = {Internet Society},
title = {{CodeAlchemist: Semantics-Aware Code Generation to Find Vulnerabilities in JavaScript Engines}},
year = {2019}
}
@article{sym:sum,
abstract = {Symbolic execution has become an effective program testing technique, providing a way to automatically generate inputs that trigger software errors ranging from low-level program crashes to higher-level semantic properties' generate test suites that achieve high program coverage' and provide per-path correctness guarantees. While more research is needed to scale symbolic execution to very large programs, existing tools have already proved effective in testing and finding errors in a variety of software, varying from low-level network and operating systems code to higher-level applications code. {\textcopyright} 2013 ACM.},
annote = {经典的符号执行综述，虽然不在推荐期刊中，但是将其保留},
author = {Cadar, Cristian and Sen, Koushik},
doi = {10.1145/2408776.2408795},
issn = {00010782},
journal = {Communications of the ACM},
month = {feb},
number = {2},
pages = {82},
title = {{Symbolic Execution for Software Testing: Three Decades Later}},
volume = {56},
year = {2013}
}
@article{sym:exe,
abstract = {This article presents EXE, an effective bug-finding tool that automatically generates inputs that crash real code. Instead of running code on manually or randomly constructed input, EXE runs it on symbolic input initially allowed to be anything. As checked code runs, EXE tracks the constraints on each symbolic (i.e., input-derived) memory location. If a statement uses a symbolic value, EXE does not run it, but instead adds it as an input-constraint; all other statements run as usual. If code conditionally checks a symbolic expression, EXE forks execution, constraining the expression to be true on the true branch and false on the other. Because EXE reasons about all possible values on a path, it has much more power than a traditional runtime tool: (1) it can force execution down any feasible program path and (2) at dangerous operations (e.g., a pointer dereference), it detects if the current path constraints allow any value that causes a bug. When a path terminates or hits a bug, EXE automatically generates a test case by solving the current path constraints to find concrete values using its own co-designed constraint solver, STP. Because EXEs constraints have no approximations, feeding this concrete input to an uninstrumented version of the checked code will cause it to follow the same path and hit the same bug (assuming deterministic code). EXE works well on real code, finding bugs along with inputs that trigger them in: the BSD and Linux packet filter implementations, the dhcpd DHCP server, the pcre regular expression library, and three Linux file systems. {\textcopyright} ACM 2008.},
author = {Cadar, Cristian and Ganesh, Vijay and Pawlowski, Peter M. and Dill, David L. and Engler, Dawson R.},
doi = {10.1145/1455518.1455522},
issn = {10949224},
journal = {ACM Transactions on Information and System Security},
keywords = {Attack generation,Bug finding,Constraint solving,Dynamic analysis,Symbolic execution,Test case generation},
month = {dec},
number = {2},
pages = {1--38},
title = {{EXE: Automatically Generating Inputs of Death}},
volume = {12},
year = {2008}
}
@inproceedings{aletheia,
abstract = {The scale and complexity of modern software systems complicate manual security auditing. Automated analysis tools are gradually becoming a necessity. Specifically, static security analyses carry the promise of efficiently verifying large code bases. Yet, a critical usability barrier, hindering the adoption of static security analysis by developers, is the excess of false reports. Current tools do not offer the user any direct means of customizing or cleansing the re- port. The user is thus left to review hundreds, if not thousands, of potential warnings, and classify them as either actionable or spuri- ous. This is both burdensome and error prone, leaving developers disenchanted by static security checkers. We address this challenge by introducing a general technique to refine the output of static security checkers. The key idea is to apply statistical learning to the warnings output by the analysis based on user feedback on a small set of warnings. This leads to an interactive solution, whereby the user classifies a small fragment of the issues reported by the analysis, and the learning algorithm then classifies the remaining warnings automatically. An important aspect of our solution is that it is user centric. The user can express different classification policies, ranging from strong bias toward elimination of false warnings to strong bias toward preservation of true warnings, which our filtering system then executes. We have implemented our approach as the ALETHEIA tool. Our evaluation of ALETHEIA on a diversified set of nearly 4,000 client- side JavaScript benchmarks, extracted from 675 popularWeb sites, is highly encouraging. As an example, based only on 200 classi- fied warnings, and with a policy biased toward preservation of true warnings, ALETHEIA is able to boost precision by a threefold fac- tor (×2.868), while reducing recall by a negligible factor (×1.006). Other policies are enforced with a similarly high level of efficacy},
address = {New York, New York, USA},
author = {Tripp, Omer and Guarnieri, Salvatore and Pistoia, Marco and Aravkin, Aleksandr},
booktitle = {Proceedings of the 2014 ACM SIGSAC Conference on Computer and Communications Security},
doi = {10.1145/2660267.2660339},
isbn = {9781450329576},
issn = {15437221},
keywords = {Classification,False alarms,Information-flow security,Machine learning,Static analysis,Usable security},
pages = {762--774},
publisher = {ACM Press},
title = {{ALETHEIA:Improving the Usability of Static Security Analysis}},
year = {2014}
}
@article{slices:ottenstein1984program,
abstract = {The internal program representation chosen for a software development environment plays a critical role in the nature of that environment. A form should facilitate implementation and contribute to the responsiveness of the environment to the user. The program dependence graph (PPG) may be a suitable internal form. It allows programs to be sliced in linear time for debugging and for use by language-directed editors. The slices obtained are more accurate than those obtained with existing methods because I/O is accounted for correctly and irrelevant statements on multi-statement lines are not displayed. The PDG may be interpreted in a data driven fashion or may have highly optimized (including vectorized) code produced from it. It is amenable to incremental data flow analysis, improving response time to the user in an interactive environment and facilitating debugging through data flow anomaly detection. It may also offer a good basis for software complexity metrics, adding to the completeness of an environment based on it.},
address = {New York, New York, USA},
author = {Ottenstein, Karl J. and Ottenstein, Linda M.},
doi = {10.1145/800020.808263},
isbn = {0897911318},
journal = {Proceedings of the first ACM SIGSOFT/SIGPLAN software engineering symposium on Practical software development environments},
keywords = {Code optimization,Control flow,Data flow,Internal program representation,Interpreter Debugging,Program slice,Software complexity metrics},
number = {5},
pages = {177--184},
publisher = {ACM Press},
title = {{The program dependence graph in a software development environment}},
volume = {19},
year = {1984}
}
@inproceedings{Kildall1973,
address = {New York, New York, USA},
annote = {数据流分析提出},
author = {Kildall, Gary A.},
booktitle = {Proceedings of the 1st annual ACM SIGACT-SIGPLAN symposium on Principles of programming languages},
doi = {10.1145/512927.512945},
pages = {194--206},
publisher = {ACM Press},
title = {{A unified approach to global program optimization}},
year = {1973}
}
@book{B:automatedTheoremProving,
abstract = {Growing demands for the quality, safety, and security of software can only be satisfied by the rigorous application of formal methods during software design. This book methodically investigates the potential of first-order logic automated theorem provers for applications in software engineering. Illustrated by complete case studies on protocol verification, verification of security protocols, and logic-based software reuse, this book provides techniques for assessing the prover's capabilities and for selecting and developing an appropriate interface architecture.},
address = {Berlin, Heidelberg},
author = {Schumann, Johann M.},
booktitle = {Automated Theorem Proving in Software Engineering},
doi = {10.1007/978-3-662-22646-9},
isbn = {978-3-642-08759-2},
publisher = {Springer Berlin Heidelberg},
title = {{Automated Theorem Proving in Software Engineering}},
year = {2001}
}
@inproceedings{largeBatchLSTM,
address = {New York, NY, USA},
annote = {批处理},
author = {You, Yang and Hseu, Jonathan and Ying, Chris and Demmel, James and Keutzer, Kurt and Hsieh, Cho-Jui},
booktitle = {Proceedings of the 2019 International Conference for High Performance Computing, Networking, Storage and Analysis},
doi = {10.1145/3295500.3356137},
isbn = {9781450362290},
month = {nov},
pages = {1--16},
publisher = {ACM},
title = {{Large-batch training for LSTM and beyond}},
year = {2019}
}
@article{sym:cute,
abstract = {In unit testing, a program is decomposed into units which are collections of functions. A part of unit can be tested by generating inputs for a single entry function. The entry function may contain pointer arguments, in which case the inputs to the unit are memory graphs. The paper addresses the problem of automating unit testing with memory graphs as inputs. The approach used builds on previous work combining symbolic and concrete execution, and more specifically, using such a combination to generate test inputs to explore all feasible execution paths. The current work develops a method to represent and track constraints that capture the behavior of a symbolic execution of a unit with memory graphs as inputs. Moreover, an efficient constraint solver is proposed to facilitate incremental generation of such test inputs. Finally, CUTE, a tool implementing the method is described together with the results of applying CUTE to real-world examples of C code. Copyright 2005 ACM.},
address = {New York, New York, USA},
author = {Sen, Koushik and Marinov, Darko and Agha, Gul},
doi = {10.1145/1081706.1081750},
isbn = {1595930140},
issn = {01635948},
journal = {Proceedings of the 10th European software engineering conference held jointly with 13th ACM SIGSOFT international symposium on Foundations of software engineering},
keywords = {Concolic testing,Data structure testing,Explicit path model-checking,Random testing,Testing C programs,Unit testing},
month = {sep},
number = {5},
pages = {263},
publisher = {ACM Press},
title = {{CUTE: a concolic unit testing engine for C}},
volume = {30},
year = {2005}
}
@article{Zou2018,
author = {邹权臣 and 张涛 and 吴润浦 and 马金鑫 and 李美聪 and 陈晨 and 侯长玉},
journal = {清华大学学报 (自然科学版)},
number = {12},
pages = {1079--1094},
title = {从自动化到智能化:软件漏洞挖掘技术进展},
volume = {58},
year = {2018}
}
@article{taint:wanglei,
author = {王蕾 and 李丰 and 李炼 and 冯晓兵},
journal = {软件学报},
number = {4},
pages = {860--882},
title = {污点分析技术的原理和实践应用},
volume = {28},
year = {2017}
}
@article{liujian2018,
author = {刘剑 and 苏璞睿 and 杨珉 and 和亮 and 张源 and 朱雪阳 and 林惠民},
journal = {软件学报},
number = {1},
pages = {42--68},
title = {软件与网络安全研究综述},
volume = {29},
year = {2018}
}
@article{meihong2009,
author = {梅宏 and 王千祥 and 张路 and 王戟},
journal = {计算机学报},
number = {9},
pages = {1697--1710},
title = {软件分析技术进展},
volume = {32},
year = {2009}
}
@article{artoffuzz,
abstract = {Among the many software testing techniques available today, fuzzing has remained highly popular due to its conceptual simplicity, its low barrier to deployment, and its vast amount of empirical evidence in discovering real-world software vulnerabilities. At a high level, fuzzing refers to a process of repeatedly running a program with generated inputs that may be syntactically or semantically malformed. While researchers and practitioners alike have invested a large and diverse effort towards improving fuzzing in recent years, this surge of work has also made it difficult to gain a comprehensive and coherent view of fuzzing. To help preserve and bring coherence to the vast literature of fuzzing, this paper presents a unified, general-purpose model of fuzzing together with a taxonomy of the current fuzzing literature. We methodically explore the design decisions at every stage of our model fuzzer by surveying the related literature and innovations in the art, science, and engineering that make modern-day fuzzers effective.},
archivePrefix = {arXiv},
arxivId = {1812.00140},
author = {Manes, Valentin Jean Marie and Han, Hyung Seok and Han, Choongwoo and sang kil Cha and Egele, Manuel and Schwartz, Edward J. and Woo, Maverick},
doi = {10.1109/TSE.2019.2946563},
eprint = {1812.00140},
file = {:D$\backslash$:/Mendeley/Manes et al/IEEE Transactions on Software Engineering/Manes et al. - 2019 - The Art, Science, and Engineering of Fuzzing A Survey.pdf:pdf},
issn = {19393520},
journal = {IEEE Transactions on Software Engineering},
keywords = {Computer bugs,Fuzzing,Security,Terminology,automated software testing,fuzz testing,fuzzing,software security},
pages = {1--1},
title = {{The Art, Science, and Engineering of Fuzzing: A Survey}},
year = {2019}
}
@article{Koc2017,
abstract = {The large scale and high complexity of modern software systems make perfectly precise static code analysis (SCA) infeasible. There-fore SCA tools often over-approximate, so not to miss any real problems. This, however, comes at the expense of raising false alarms, which, in practice, reduces the usability of these tools. To partially address this problem, we propose a novel learning process whose goal is to discover program structures that cause a given SCA tool to emit false error reports, and then to use this information to predict whether a new error report is likely to be a false positive as well. To do this, we first preprocess code to isolate the locations that are related to the error report. Then, we apply machine learning techniques to the preprocessed code to discover correlations and to learn a classifier. We evaluated this approach in an initial case study of a widely-used SCA tool for Java. Our results showed that for our dataset we could accurately classify a large majority of false positive error reports. Moreover, we identified some common coding patterns that led to false positive errors. We believe that SCA developers may be able to redesign their methods to address these patterns and reduce false positive error reports.},
author = {Koc, Ugur and Saadatpanah, Parsa and Foster, Jeffrey S. and Porter, Adam A.},
doi = {10.1145/3088525.3088675},
file = {:D$\backslash$:/Mendeley/Koc et al/Proceedings of the 1st ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, co-located with PLDI 2017/Koc et al. - 2017 - Learning a classifier for false positive error reports emitted by static code.pdf:pdf},
isbn = {9781450350716},
journal = {Proceedings of the 1st ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, co-located with PLDI 2017},
keywords = {Long short-term memories,Naive bayes classifier,Program slicing,Static code analysis},
pages = {35--42},
title = {{Learning a classifier for false positive error reports emitted by static code analysis tools}},
year = {2017}
}
